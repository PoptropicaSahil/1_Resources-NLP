# Notes from Cameron Wolfe's substack titled 'Basics of Reinforcement Learning for LLMs'

<https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning>

**When is RL useful?** We **cannot apply a loss function** that trains the language model to maximize human preferences with supervised learning. Why? Well, the **score that we get from the human is a bit of a black box.** There’s no easy way for us to explain this score or connect it mathematically to the output of the neural network.

> In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text.

<img src="readme-images/environ-diff.png" alt="drawing" width="700"/>

**Big picture.** RL allows us to learn from signals that are non-differentiable and, therefore, not compatible with supervised learning. **Put simply, this means that we can learn from arbitrary feedback on a neural network’s output!** In the case of RLHF, we can score the outputs generated by a language model according to any principle that we have in mind. Then, we can use **RL to learn from these scores,** no matter how we choose to define them! **In this way, we can teach a language model to be helpful, harmless, honest, more capable (e.g., by using tools), and much more.**

## Formal Framwork

<img src="readme-images/rl-intro.png" alt="drawing" width="700"/>

- An **agent** that is interacting with an **environment**. The agent has a **state** in the environment and produces **actions**, which can modify the current state, as output. As the agent interacts with the environment, it can receive both positive and negative **rewards** for its actions. The agent’s goal is to maximize the rewards that it receives, but there is **not a reward associated with every action taken by the agent!** Rather, rewards may *have a long horizon*, meaning that it takes several correct, consecutive actions to generate any positive reward.

## Markov Decision Process (MDP)

<img src="readme-images/mdp.png" alt="drawing" width="500"/>

- States and actions have discrete values *since finite set?*
- Rewards are real numbers 

In an MDP, we define two types of functions: (i) **transition** and (ii) **policy** functions. 
- The **policy** takes a state as input, then outputs a *probability distribution over possible actions.* 
- Given this output, we can *make a decision for the action* to be taken from a current state
- The **transition** is then a function that *outputs the next state based upon the prior state and chosen action.* 

Using these components, the agent can interact with the environment in an iterative fashion

<img src="readme-images/mdp-structure.png" alt="drawing" width="700"/>

> Think of the agent as implementing the policy within its environment. The policy describes how the agent chooses its next action given the current state. The agent follows this strategy as it interacts with the environment, and our goal is to learn a policy that maximizes the reward that the agent receives from the environment.

As the agent interacts with the environment, we form a **trajectory** of states and actions that are chosen throughout this process. Then, given the reward associated with each of these states, we get a total return given by the equation below, where **$γ$** is the **discount factor**. This return is the summed reward across the agent’s full trajectory, but **rewards achieved at later time steps are exponentially discounted by the factor $γ$**

<img src="readme-images/trajectory-return.png" alt="drawing" width="500"/>


The goal of RL is to **train an agent** that **maximizes this return**. Equated as **finding a policy that maximizes the return** over trajectories that are sampled from the final policy

<img src="readme-images/objective.png" alt="drawing" width="500"/>


## RL for LLMs

At inference, the LM 
1. Predicts the next token.
1. Adds the next token to the current input sequence.
1. Repeats.

To view this setup from the lens of RL
- Consider our **language model to be the policy.** 
- **State is the current textual sequence.** 
- Given this state as input, the language model (policy) can **produce an action—the next token** — that modifies the current state to produce the **next state—the textual sequence with an added token**. 
- Once a full textual sequence has been produced, we can obtain a **reward by rating the quality of the language model’s output**, either with a *human or a reward model* that has been trained over human preferences.

> OOOOOOFFFFFFF!!

We begin to see that the problem formulation used for RL is quite generic. There are many different problems that we can solve using this approach!
