# Notes from Cameron Wolfe's substack titled 'Basics of Reinforcement Learning for LLMs'

<https://cameronrwolfe.substack.com/p/basics-of-reinforcement-learning>

**When is RL useful?** We **cannot apply a loss function** that trains the language model to maximize human preferences with supervised learning. Why? Well, the **score that we get from the human is a bit of a black box.** There’s no easy way for us to explain this score or connect it mathematically to the output of the neural network.

> In other words, we cannot backpropagate a loss applied to this score through the rest of the neural network. This would require that we are able to differentiate (i.e., compute the gradient of) the system that generates the score, which is a human that subjectively evaluates the generated text.

<img src="readme-images/environ-diff.png" alt="drawing" width="500"/>

**Big picture.** RL allows us to learn from signals that are non-differentiable and, therefore, not compatible with supervised learning. **Put simply, this means that we can learn from arbitrary feedback on a neural network’s output!** In the case of RLHF, we can score the outputs generated by a language model according to any principle that we have in mind. Then, we can use **RL to learn from these scores,** no matter how we choose to define them! **In this way, we can teach a language model to be helpful, harmless, honest, more capable (e.g., by using tools), and much more.**

## Formal Framwork

<img src="readme-images/rl-intro.png" alt="drawing" width="500"/>

- An **agent** that is interacting with an **environment**. The agent has a **state** in the environment and produces **actions**, which can modify the current state, as output. As the agent interacts with the environment, it can receive both positive and negative **rewards** for its actions. The agent’s goal is to maximize the rewards that it receives, but there is **not a reward associated with every action taken by the agent!** Rather, rewards may *have a long horizon*, meaning that it takes several correct, consecutive actions to generate any positive reward.

## Markov Decision Process (MDP)

<img src="readme-images/mdp.png" alt="drawing" width="500"/>
