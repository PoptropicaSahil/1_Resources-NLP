{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Pretraining Language Models\n",
    "\n",
    "There are many pretrained BERT models that can be used \"off-the-shelf\".  However, there are times when it is advantageous to train or fine-tune a new language model for downstream NLP tasks.  For example, medical papers use vocabularies that are specific to the medical domain, so a language model trained on medical papers will be better suited to projects that process medical text than one trained on more general text.  \n",
    "\n",
    "In this notebook, you'll learn how to pretrain a BERT language model with domain-specific data.  \n",
    "    \n",
    "**[3.1 Data Preparation](#3.1-Data-Preparation)<br>**\n",
    "**[3.2 Training the BERT Tokenizer](#3.2-Training-the-BERT-Tokenizer)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 Exercise: Tokenize a Term](#3.2.1-Exercise:-Tokenize-a-Term)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.2 Update the BERT Vocabulary](#3.2.2-Update-the-BERT-Vocabulary)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.3 Exercise: Train a Larger Vocabulary](#3.2.3-Exercise:-Train-a-Larger-Vocabulary)<br>\n",
    "**[3.3 Launch BERT Pretraining with NeMo](#3.3-Launch-BERT-Pretraining-with-NeMo)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.1 TensorBoard Visualization](#3.3.1-TensorBoard-Visualization)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.2 Practical Considerations](#3.3.2-Practical-Considerations)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.1 Data Preparation\n",
    "\n",
    "Masked neural language models, such as BERT, are trained on text.  However, the text must first be transformed into numerical representations, a process called tokenization.  The network is then trained by masking random words in the input sentence and predicting the missing words.  The trained language model can then be used in downstream NLP tasks, where it is referred to as a \"pretrained\" language model.\n",
    "\n",
    "With NVIDIA NeMo, the tokenization can be done either on-the-fly during training or offline before training.\n",
    "\n",
    "- **On-the-fly data preprocessing:** The training and validation text files should have words separated by spaces:\n",
    "                                [WORD] [SPACE] [WORD] [SPACE] [WORD] [SPACE] [WORD]\n",
    "                                \n",
    "- **Offline data preprocessing:** Data is prepared in advance in HD5F format. This is the recommended preprocessing for large text corpora.  Refer to [BERT quick start guide](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT#quick-start-guide) for the offline data preprocessing script. \n",
    "\n",
    "In our example, we will use the on-the-fly data preprocessing pipeline.  We'll train BERT on the [NCBI-disease corpus](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/).\n",
    "The NCBI corpus is a set of 793 PubMed abstracts.  Our goal is to create a pretrained model for the medical domain.  Here's an example of text abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -5 /dli/task/data/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.2 Training the BERT Tokenizer\n",
    "\n",
    "As discussed in the previous notebook, the BERT tokenizer splits the text into tokens following a predefined vocabulary. The tokenizer algorithm generates the vocabulary following variants of Top-K frequent words from text corpus.\n",
    "\n",
    "The vocabulary size is limited because the training cost increases with the size of the vocabulary. Including all unique words from the text corpus into the vocabulary would explode the complexity of training beyond the capabilities of the tokenizer. For instance, the BERT model that was released in 2018, with a subword tokenizer algorithm called WordPiece, has a vocabulary limit of 30,000.\n",
    "\n",
    "How, then, do tokenizers deal with terms that are not part of the vocabulary, or **out-of-vocabulary (OOV)** words?\n",
    "\n",
    "1. One option is to replace OOV words with a special token \\[UNK\\]. In this case, all OOV terms will have the same representation for the neural network loosing the semantic. \n",
    "1. A second option is to split OOV words at the character level. This increases the size of the input to the neural language model, adding the challenge of learning the relationship between characters to keep the semantic.\n",
    "1. Sub-word tokenizers, such as BERT WordPiece, provide a solution in between the word token and character split option. It tokenizes OOV words into subwords.\n",
    "\n",
    "Let's have a look at the `bert-base-uncased` tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nemo nlp collection \n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# load the bert-base-uncased tokenizer \n",
    "tokenizer_uncased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocabulary size\n",
    "print(\" The vocabulary size: \", tokenizer_uncased.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, take a look at the format tokenization for years with BERT. Years prior to 2021 appear frequently enough in the corpus to be part of the vocabulary, while years in the future are OOV and are split into sub-tokens.\n",
    "\n",
    "Try it in the cell below using the `tokenizer_uncased.text_to_tokens()` function for various years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert tokenizer for years\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2019'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2020'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2021'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2022'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2023'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2030'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The years tokenization example gives us some intuition into the process.  How about domain-specific context such as medical jargon? For a concrete example, try again with the following sentence:\n",
    "\n",
    "_\"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\"_\n",
    "\n",
    "This sentence includes several medical terms such as dilutions, C5D, C5, hemolytic and assay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert tokenizer for domain-specific example\n",
    "SAMPLES = \"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor.\"\n",
    "print(\"Tokenized sentence: \", tokenizer_uncased.text_to_tokens(SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see medical jargon tokenized as subwords: \n",
    "- dilutions -> 'dil', '##ution', '##s'\n",
    "- hemolytic ->'hem', '##ol', '##ytic'\n",
    "- assay -> 'ass', '##ay'\n",
    "- C5 ->'c', '##5'\n",
    "- C5D ->'c', '##5', '##d'\n",
    "\n",
    "The medical jargon such as dilutions, hemolytic and assay are not in the standard BERT tokenizer vocabulary. Therefore, they cannot be individually tokenized and are divided into subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Exercise: Tokenize a Term\n",
    "Correct the \"FIXME\" lines below to tokenize the term \"COVID-19\" using the BERT tokenizer.  Check the [solution](solutions/ex3.2.1.ipynb) if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a new term\n",
    "TEXT = \"\" #FIXME\n",
    "print(\"Tokenized sentence: \") #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Update the BERT Vocabulary\n",
    "\n",
    "It is possible to add domain specific words into the tokenizer vocabulary with the `tokenizer_uncased.tokenizer.add_tokens()` function. The embeddings vector for each new token will be initialized with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some medical jargon to the vocabulary of Bert tokenizer\n",
    "additional_tokens = tokenizer_uncased.tokenizer.add_tokens([\"dilutions\", \"hemolytic\"])\n",
    "print(\" The vocabulary size before: \", tokenizer_uncased.vocab_size)\n",
    "print(\" The vocabulary size after : \", tokenizer_uncased.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence with the new vocabulary \n",
    "print(\"Tokenized sentence: \", tokenizer_uncased.text_to_tokens(SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of domain-specific words to incorporate into the vocabulary is high, it is the best to train a new tokenizer from a domain-specific corpus, rather than to use the pretrained tokenizer. \n",
    "\n",
    "Let's train a new WordPiece tokenizer on the [NCBI-disease corpus] corpus, limiting the vocabulary size to 10,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "text_corpus=[\"/dli/task/data/train.txt\"]\n",
    "\n",
    "# add the special tokens required for BERT pretraining.\n",
    "special_tokens = [\"<PAD>\",\"<UNK>\",\"<CLS>\",\"<SEP>\",\"<MASK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "my_bert_tokenizer = BertWordPieceTokenizer()\n",
    "my_bert_tokenizer.train(files=text_corpus, vocab_size=vocab_size,\n",
    "                        min_frequency=1, special_tokens=special_tokens,\n",
    "                        show_progress=True, wordpieces_prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the new vocabulary size\n",
    "print(\" The new vocabulary size  : \", len(my_bert_tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new vocabulary \n",
    "my_bert_tokenizer.save_model(directory=\"/dli/task/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -20 /dli/task/data/vocab.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vocabulary is defined, we can load the tokenizer with the new vocabulary using the `nemo_nlp.modules.get_tokenizer()` function. Let's tokenize the previous text sample and compare to the vanilla BERT tokenizer. \n",
    "The domain-specific jargon should now be encoded as individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer from the vocabulary \n",
    "special_tokens_dict = {\"unk_token\": \"<UNK>\", \"sep_token\": \"<SEP>\", \"pad_token\": \"<PAD>\", \"bos_token\": \"<CLS>\", \"mask_token\": \"<MASK>\",\"eos_token\": \"<SEP>\", \"cls_token\": \"<CLS>\"}\n",
    "tokenizer_custom = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\", vocab_file='/dli/task/data/vocab.txt', special_tokens=special_tokens_dict)\n",
    "\n",
    "print(\"BERT tokenizer with custom vocabulary: \", tokenizer_custom.text_to_tokens(SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Exercise: Train a Larger Vocabulary \n",
    "\n",
    "Correct the \"FIXME\" lines to train a BERT tokenizer with a vocabulary size of 15,000. Check the [solution](solutions/ex3.2.3.ipynb) if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train a larger vocabulary \n",
    "vocab_size = None #FIXME\n",
    "my_bert_tokenizer_15k= None #FIXME\n",
    "my_bert_tokenizer_15k.train(files=text_corpus, vocab_size=vocab_size, \n",
    "                            min_frequency=1, special_tokens=special_tokens, \n",
    "                            show_progress=True, wordpieces_prefix=\"##\")\n",
    "print(\" The new vocabulary size  : \", len(my_bert_tokenizer_15k.get_vocab()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.3 Launch BERT Pretraining with NeMo\n",
    "\n",
    "We will use the model configuration for on-the-fly data preprocessing, [bert_pretraining_from_text_config.yaml](nemo/examples/nlp/language_modeling/conf/bert_pretraining_from_text_config.yaml), along with a training script, [bert_pretraining.py](nemo/examples/nlp/language_modeling/bert_pretraining.py). The YAML configuration file provides the parameters needed by the training script, and the parameter values can be overridden as needed. \n",
    "\n",
    "You'll learn more about NeMo configuration files and scripts in a later module.  For now, we'll just note a few important YAML keys in the configuration file:\n",
    "- `trainer`: Training process parameters such as the number of GPUs, Mixed precision training, number of epochs, etc.\n",
    "- `model.only_mlm_loss`: Use masked language model without next sentence prediction\n",
    "- `model.mask_prob`: Probability of masking a token in the input text during data processing\n",
    "- `model.train_ds`/`model.validation_ds`: datasets parameters\n",
    "- `model.tokenizer`: tokenizer parameters\n",
    "- `model.language_model`: language model architecture parameters\n",
    "- `model.optim`: Optimizer parameters\n",
    "\n",
    "Find more details about bert_pretraining parameters in the [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/bert_pretraining.html#quick-start-guide).\n",
    "\n",
    "For BERT offline pretraining with preprocessed data, use the dedicated configuration, [bert_pretraining_from_preprocessed_config.yaml](nemo/examples/nlp/language_modeling/conf/bert_pretraining_from_preprocessed_config.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the configuration file\n",
    "! cat nemo/examples/nlp/language_modeling/conf/bert_pretraining_from_text_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Override the parameters specific to our data; run only two epochs for now\n",
    "! python nemo/examples/nlp/language_modeling/bert_pretraining.py \\\n",
    "    model.train_ds.data_file=/dli/task/data/train.txt\\\n",
    "    model.validation_ds.data_file=/dli/task/data/test.txt\\\n",
    "    model.tokenizer.vocab_file=/dli/task/data/vocab.txt\\\n",
    "    model.train_ds.batch_size=16 \\\n",
    "    trainer.max_epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 TensorBoard Visualization\n",
    "Execute the next cell to create a link to TensorBoard in your browser.  Then, click the link to see graphs of experiment metrics like loss and accuracy saved in the `nemo_experiments` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Practical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining a Transformer-based language models does not require labeled text corpus datasets. However, it does require a large amount of data and compute time.  For example, pretraining a BERT model on the [English Wikipedia](https://huggingface.co/datasets/wikipedia) + [bookcorpus](https://huggingface.co/datasets/bookcorpus) using an NVIDIA DGX-1 server with 8 V100 GPUs takes about 6 days in mixed precision mode. You can find out more about BERT training and fine-tuning performance at https://catalog.ngc.nvidia.com/orgs/nvidia/resources/bert_for_pytorch/performance.\n",
    "\n",
    "On the other hand, fine-tuning a Transformer-based model is less computationally intensive, but requires labeled data. The lab in Part 2 will focus on fine-tuning BERT models for downstream NLP tasks such as text classification and named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've completed the BERT pretraining notebook!  \n",
    "\n",
    "You've learned:\n",
    "* How to train a BERT tokenizer\n",
    "* How to pretrain a BERT language model with NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "186px",
    "left": "619px",
    "top": "238px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
