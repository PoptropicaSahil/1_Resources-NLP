{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcadef10-3db2-42c9-818d-f0968e529669",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da412f2d-e0e2-4c5d-ae79-2802ce449dc0",
   "metadata": {},
   "source": [
    "# 2.0 BERT Overview\n",
    "In this notebook, you'll look deeper into BERT architecture and contextualized word embeddings. You'll get a chance to visualize how words in a pretrained language model relate to each other.\n",
    "\n",
    "**[2.1 Introduction to BERT](#2.1-Introduction-to-BERT)<br>**\n",
    "**[2.2 BERT Language Model with NeMo](#2.2-BERT-Language-Model-with-NeMo)<br>**\n",
    "**[2.3 The BERT WordPiece Tokenizers](#2.3-The-BERT-WordPiece-Tokenizers)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.1 Characters, Words, Subwords](#2.3.1-Characters,-Words,-Subwords)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.2 WordPiece Algorithm](#2.3.2-WordPiece-Algorithm)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.3 Tokenizer with NeMo](#2.3.3-Tokenizer-with-NeMo)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.4 Exercise: Tokenizer Index](#2.3.4-Exercise:-Tokenizer-Index)<br>\n",
    "**[2.4 Contextualized Word Embedding](#2.4-Contextualized-Word-Embedding)<br>**\n",
    "**[2.5 The Attention Mechanism](#2.5-The-Attention-Mechanism)<br>**\n",
    "\n",
    "\n",
    "We are focusing on understanding BERT architecture in preparation for pretraining language models.  Pretraining is covered in the next notebook of this lab.  Note that the training process for even relatively small language model variants, such as BERT Base trained on Wikipedia, is very time-consuming.  This would significantly exceed the time we have for today's class. Therefore, we will only initialize the training process and will not be able to run it to completion.\n",
    "\n",
    "The NLP fine-tuning tasks, which make use of pretrained language models, are covered in part 2 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82632005-5f86-42e6-ac9a-f6ff5af9c8e3",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.1 Introduction to BERT\n",
    "The Transformer model, introduced in [\"Attention is All You Need!\"(Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762), initially for the Neural Machine Translation (NMT) task, is an encoder-decoder architecture that relies on an attention mechanism. \n",
    "\n",
    "**BERT**, which stands for **B**idirectional **E**ncoder **R**epresentations from **T**ransformers, is a model based on the **encoder** part of Transformer model. BERT encodes input text by mapping each word or token into a trained, contextualized representation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a812f1f-86ff-43c1-a0ad-ce9e355631e9",
   "metadata": {},
   "source": [
    "<center><figure>\n",
    "    <img src=\"images/From_Transformer_To_Bert_architecture.png\">\n",
    "    <figcaption>Figure 1. Image Credit: <a href=\"https://arxiv.org/abs/1706.03762\">Attention is All You Need</a></figcaption>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142131c-0115-469a-ba2a-706386db47d4",
   "metadata": {},
   "source": [
    "In particular, the BERT model takes, as input, two sentences separated by a special token [SEP], and is pretrained using two loss functions:\n",
    " - Masked-language model prediction \n",
    " - Next sentence prediction\n",
    "\n",
    "To convert raw text into a numerical representation, the BERT model uses a subword tokenization algorithm named WordPiece. \n",
    "\n",
    "BERT is often used as a language model encoder. Its pretrained checkpoint is extended with additional layers, which are task specific (see Figure 2). They are then fine-tuned on downstream tasks such as token classification (named entity recognition), text classification, question answering, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f62fa-3c20-49eb-bf96-585ce236930d",
   "metadata": {},
   "source": [
    "<center><figure>\n",
    "    <img src=\"images/BERT.PNG\">\n",
    "    <figcaption>Figure 2. Image credit: <a href=\"https://arxiv.org/pdf/1810.04805.pdf\">BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "        Language Understanding</a>.</figcaption>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c75bf-a057-4890-a466-21d9d02a1299",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.2 BERT Language Model with NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf59e3e-be10-4d17-b437-0b095561bee3",
   "metadata": {},
   "source": [
    "BERT is a widely used neural network, with countless implementations and publicly available pretrained checkpoints. \n",
    "\n",
    "In our examples, we'll use BERT pretrained models from the [NVIDIA NeMo \\(Neural Modules\\) Toolkit](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html#neural-modules).  NeMo is a deep learning framework based on [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning).  NeMo will be covered in depth in the Part 2 lab, but for now we'll just jump right in. We will begin by importing the necessary dependencies and listing the variants of BERT available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1ba7e-be46-401b-be12-a5fd167c6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nemo nlp collection \n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# Import BERT\n",
    "from nemo.collections.nlp.models import BERTLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb449319-ae5b-402d-ab54-c27770451a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the list of pre-trained BERT language models\n",
    "BERTLMModel.list_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0373527-df71-476e-a7f3-4b6f3a1360d3",
   "metadata": {},
   "source": [
    "There are two pretrained BERT language models available with NeMo: \n",
    "- `bertbaseuncased` model has 110 millions parameters in total with 12 Transformer blocks.\n",
    "- `bertlargeuncased` model has 340 millions parameters in total with 24 Transformer blocks.\n",
    "\n",
    "For the sake of time and simplicity, we'll download the smaller variant, BERT Base. This could take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beaa803-2fc1-439a-be16-689825c3c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pretrained BERT-based model\n",
    "pretrained_model_name=\"bertbaseuncased\"\n",
    "model = BERTLMModel.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145e59b-db6d-4f5d-9459-75d9130fef86",
   "metadata": {},
   "source": [
    "Execute the next cell to inspect the architecture of the model. Please compare the output listed with Figure 1. \n",
    "* Can you identify all of the layers? \n",
    "* How many Transformer layers does BERT Base have? \n",
    "* How big is the hidden layer size? Can you identify the Key, Value and Query matrices? \n",
    "* Can you find two loss components, one responsible for Next Sentence Prediction and one for the Masked Language Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcae0b2-a14a-4327-a697-95702b46db93",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee5b56-001f-4650-a8ef-36a11f4dabe2",
   "metadata": {},
   "source": [
    "Inspect the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788a855-0ccf-40fd-8207-c240e0a16c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of weights\n",
    "print(\" Number of weights : \",model.num_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcafb97-4ee9-466e-ba97-15ed3be2cf0a",
   "metadata": {},
   "source": [
    "# 2.3 The BERT WordPiece Tokenizers\n",
    "\n",
    "Tokenization is an important data preprocessing step that consists of converting text raw data into discrete numerical representations required for the neural language model. There are several tokenizer algorithms that split the text into tokens based on rules for characters, words, or subwords. The size of the vocabulary is determined by the algorithm and dependent upon the frequency of tokens found in the text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f24ca-7573-4b46-b1a7-d2cbe4a94b04",
   "metadata": {},
   "source": [
    "## 2.3.1 Characters, Words, Subwords\n",
    "Tokenization splits a word, phrase, or larger text section into individual characters, words, or subwords.  For example, the word \"tokenization\" could be split in a number of ways:\n",
    "\n",
    "* Characters: 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'\n",
    "* Words: 'tokenization'\n",
    "* Subwords: 'token', '##ization'\n",
    "\n",
    "The idea is to create a vocabulary of tokens from a text corpus, which can then be trained in a language model to characterize language relationships between the tokens.  Whether this is done by character, word, or subword affects the complexity of the problem.\n",
    "\n",
    "Tokenization by characters has the advantage of a very limited number of tokens to deal with, but these few tokens are not very meaningful by themselves and long sequences of tokens are required to represent text.  Tokenization by words results in a very large vocabulary size and requires separate tokens for very similar words, which in turn requires more training to determine their relationships to each other.\n",
    "\n",
    "Tokenization by subwords is a solution that tries to balance these two. For example, the word \"token\" is a subword for \"tokenization\", \"tokens\", and \"tokenize\".  By splitting the words, the model learns similar meanings from the same root word more easily.  The size of the overall vocabulary required for understanding is less than required for word tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f2a50-f72a-4a35-a044-8644ca0e7293",
   "metadata": {},
   "source": [
    "## 2.3.2 WordPiece Algorithm\n",
    "The WordPiece algorithm was introduced in [this paper by Schuster and Nakajima](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf).  To begin, the training data (corpus) is chosen, as well as the subword vocabulary size desired.   The algorithm iteratively determines optimal subwords for the body of text and creates the vocabulary with assigned values.  The iterative steps are:\n",
    "\n",
    "1. Split words into sequences of character tokens.\n",
    "2. Build the language model on the training data using tokens from previous step.\n",
    "3. Generate new unit tokens by combining two tokens with high likelihood in the language model and add the new token(s) to the vocabulary.\n",
    "4. Repeat from step 2 until the token limit for the desired vocabulary is reached or the likelihood falls below some desired threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7689b-4130-46ba-b705-64eee71f63b0",
   "metadata": {},
   "source": [
    "## 2.3.3 Tokenizer with NeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2492a-53cf-458c-86e4-79bac52f9e17",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check available tokenizers\n",
    "nemo_nlp.modules.get_tokenizer_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f120b-e0cc-41ee-a67b-dae97c3d29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bert-base-uncased tokenizer \n",
    "tokenizer_uncased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c6662-6abb-48b2-bb06-ee6897306a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the vocabulary size\n",
    "print(\" The vocabulary size: \", tokenizer_uncased.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8ca3c-b5e0-48ba-95a3-b966b0698475",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_TEXT_1 = \"Hello, my name is John. I live in Santa Clara.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5fd1c-676a-4e1f-9aea-99d3fb6a8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_uncased=tokenizer_uncased.text_to_tokens(SAMPLES_TEXT_1)\n",
    "print(\"Input sentence: \", SAMPLES_TEXT_1)\n",
    "print(\"Tokenized sentence: \", output_uncased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3f1c6e-46a9-4d55-8e67-9bcb1efc9076",
   "metadata": {},
   "source": [
    "Now, let's use the `bert-base-cased` tokenizer to encode the sentence, \"Hello, my name is John. I live in Santa Clara\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99338b-f85b-4644-bef4-1ecdbaa3c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bert-base-cased tokenizer \n",
    "tokenizer_cased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7d5ad-494e-4c2c-ad75-649b810165b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text \n",
    "output_cased=tokenizer_cased.text_to_tokens(SAMPLES_TEXT_1)\n",
    "print(\"Input sentence: \", SAMPLES_TEXT_1)\n",
    "print(\"Tokenized sentence: \", output_cased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492c5b2-2da2-40b2-aa06-e628b34ca543",
   "metadata": {},
   "source": [
    "The BERT model does not accept text inputs, but rather their numerical index representations.\n",
    "\n",
    "We can check the vocabulary index of a word using the `tokenizer.text_to_ids()` function. \n",
    "\n",
    "Try it with the `bert-base-cased` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdb88f-6a1d-4c9d-9f65-bc6c8ab1d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the tokens Hello and hello using bert-base-cased tokenizer\n",
    "print(\"Index of Hello: \", tokenizer_cased.text_to_ids(\"Hello\"))\n",
    "print(\"Index of hello: \",tokenizer_cased.text_to_ids(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d4f85-cf30-4602-a189-080f45ed425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of bert-base-cased tokenizer in a sentence\n",
    "print(\"Input sentence: \", SAMPLES_TEXT_1)\n",
    "print(\"Tokenized sentence: \", output_cased)\n",
    "print(\"Tokenized sentence: \", tokenizer_cased.text_to_ids(SAMPLES_TEXT_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdbf92d-05cb-4f27-839b-5fd6260d73a6",
   "metadata": {},
   "source": [
    "## 2.3.4 Exercise: Tokenizer Index\n",
    "We've already set up `tokenizer_uncased` earlier in the notebook. In the cell below, print the index for \"hello\" and \"Hello\" for `bert-base-uncased`.  If you get stuck, check the [solution](solutions/ex2.3.4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446abb94-4ac5-454c-be42-daf9b439f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the tokens Hello and hello using bert-base-uncased tokenizer\n",
    "print(\"Index of Hello: \") #FIXME\n",
    "print(\"Index of hello: \") #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f8146-b79d-42e1-90ab-3928e4978ff0",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.4 Contextualized Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd307804-8dd7-48e1-b8a4-beee948ed56d",
   "metadata": {},
   "source": [
    "The BERT model, once trained, provides relationships between the tokenized words, based on their use together in the language corpus.  These relationships are defined within the hidden states of the neural network, the _contextualized word embeddings_.  It is these relationships that can be used to solve NLP tasks such as text classification, named entity recognition, question answering, and so on.  \n",
    "To visualize the embeddings, let's start with a sentence that uses the word \"mouse\" in more than one way, and see what a pretrained model can make of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de048aa6-a7c2-41f1-a63d-5d97176a2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up the sentence we want to look at\n",
    "TEXT = \"Last night, my wireless mouse was eaten by an animal such as mouse or rat. I need to order a new optical computer mouse.\"\n",
    "input_sentence=torch.tensor([tokenizer_uncased.tokenizer(TEXT).input_ids]).cuda()\n",
    "attention_mask=torch.tensor([tokenizer_uncased.tokenizer(TEXT).attention_mask]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a986e3e-7dde-4c66-b5c9-63b0f2b9065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tokenization for the sentence\n",
    "print(\"Input sentence: \", TEXT)\n",
    "output_uncased=tokenizer_uncased.ids_to_tokens(tokenizer_uncased.tokenizer(TEXT).input_ids)\n",
    "print(\"Tokenized sentence: \", output_uncased)\n",
    "\n",
    "# \"mouse\" tokens positions in the TEXT input\n",
    "mouse_computer_1=6\n",
    "mouse_animal=14\n",
    "mouse_computer_2=26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a935db-381d-4e60-9693-0e105406a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings for the pretrained model\n",
    "hidden_states = model.bert_model(input_ids=input_sentence, token_type_ids=None, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2278a-803c-4527-ba73-036943eb0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def similarity_cosine(x,y):\n",
    "    return dot(x,y)/(norm(x)*norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cffa5b-162b-4d60-a936-614b43bd4d20",
   "metadata": {},
   "source": [
    "We can visualize text token embeddings obtained from the BERT models if we first reduce the dimensionality to 2D. \n",
    "[t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) is a dimensionality reduction technique widely used for vector visualization in 2D or 3D, as it preserves the neighborhood distances.\n",
    "\n",
    "This figure shows an example of a plot with the TEXT token BERT embeddings after dimensionality reduction to 2D dimensions using t-SNE.\n",
    "<img src=\"images/Embeddings.PNG\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859ba90-69e5-4da3-9d46-d078178d430e",
   "metadata": {},
   "source": [
    "Try it yourself!  \n",
    "\n",
    "The following codeblock uses the [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) implementation of the t-SNE algorithm to reduce the dimensionality of BERT text token embeddings from 768 to 2, and plots the 2D vectors. Note that as t-SNE is a stochastic process, the low dimensional embeddings will vary from one run to another. However, the neighborhood distances of tokens should remain more or less the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345cf431-8520-454b-b1ec-1f14e724d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "X=hidden_states.cpu().detach().numpy()\n",
    "X_embedded = TSNE(n_components=2,metric='euclidean',  init='random', perplexity=7).fit_transform(X[0])\n",
    "Tokens=tokenizer_uncased.ids_to_tokens(tokenizer_uncased.tokenizer(TEXT).input_ids)\n",
    "\n",
    "# Annotate the different mouse tokens\n",
    "Tokens[mouse_computer_1]=\"mouse_computer_1\"\n",
    "Tokens[mouse_animal]=\"mouse_animal\"\n",
    "Tokens[mouse_computer_2]=\"mouse_computer_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec67478-666d-40d2-a702-0bdc58dcf248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(X_embedded[:,0],X_embedded[:,1], '.', color='black')\n",
    "plt.plot([X_embedded[6,0],X_embedded[14,0]],[X_embedded[6,1],X_embedded[14,1]],color='red')\n",
    "plt.plot([X_embedded[6,0],X_embedded[26,0]],[X_embedded[6,1],X_embedded[26,1]],color='green')\n",
    "\n",
    "for i, txt in enumerate(Tokens):\n",
    "    plt.annotate(txt, (X_embedded[i,0], X_embedded[i,1]), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b161b0f-cd09-440c-b58d-8b132cd21a6f",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.5 The Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f6aba-9b6f-4ca4-bab1-b524045d4cef",
   "metadata": {},
   "source": [
    "As discussed in the previous notebook on the Transformer architecture, the attention mechanism allows our model to *attend*, or focus, on certain parts of the sequence, assigning higher values to parts that are more related. Despite being inspired by the human attention mechanism, the implementation in Transformers is rather simple.  It consists of multiplying our Key and Query matrices. It also involves multiple heads per layer (12 heads per layer for BERT Base), none of which are in any way conditioned to provide human-interpretable results. As a consequence, interpreting the attention mechanism can be challenging.  The neural network, across all of its layers and all of the heads, learns a very large number of text interactions and patters. We will try to visualize it, nevertheless using a tool called [BertViz](https://github.com/jessevig/bertviz).\n",
    "\n",
    "In this section, we will deliberately construct sentences that have complex, multi-token terms, like \"european economic area\" (reference: https://www.gov.uk/eu-eea), to exaggerate some of the attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44fc02-7b26-4f9b-9f93-f580cdd6ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import head, KVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551c3e7-0a72-4c1e-b91b-1e196941b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = \"The European Union (EU) is an economic and political union of 27 countries.\"\n",
    "sentence_b = \"The European Economic Area (EEA) The EEA includes EU countries and also Iceland, Liechtenstein and Norway.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c2c45-5a04-4860-ac6f-9a8baaae7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the head attention visualization tool - this may take a few minutes\n",
    "head.berthead(sentence_a,sentence_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956784e3-8094-423b-a8cc-8ce46a944817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the KVQ matrix visualization tool - this may take a few minutes\n",
    "KVQ.bertKVQ(sentence_a, sentence_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44c34a-5b7a-40e2-a53d-ebee0e61e99a",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've learned that \n",
    "* BERT is a language model based on the Transformer encoder\n",
    "* The WordPiece tokenizer converts subwords to mathematical representations\n",
    "* The relationship between words in a BERT model can be visualized by reducing the embeddings to two dimensions and plotting them\n",
    "* The attention mechanism focuses on certain parts of a sequence by assigning higher values\n",
    "\n",
    "Next, you'll learn the basics of how to tokenize and train a BERT model.  Move on to  [3.0 Pretraining Language Models](030_PretrainingLM.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9819fd-3a34-465f-96c0-8510596667d7",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
