{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Positional Encoding \n",
    "\n",
    "Language models need to make use of the sequential nature of words in a sentence. Since the Transformer model contains no recurrent or convolutional units, positional encodings (PEs) are used to account for the order of the words in the input sequence. The positional encodings have the same dimension, d<sub>model</sub>, as the embeddings, so that the two can be summed (see Figure 5). This allows the model to understand the position of each word in the input text.\n",
    "\n",
    "In the paper, the authors use sine and cosine functions of different frequencies for positional encoding:\n",
    "\n",
    "\n",
    "<img src=\"../images/pe.png\" width=\"400\">\n",
    "where <i>pos</i> is the position and <i>i</i> is the dimension with range [0, d<sub>model</sub>/2). Let's explain the formula above via an example:\n",
    "\n",
    "Let's assume that  d<sub>model</sub> = 4. This means that word ùë§ at input sequence position <i>pos</i> ‚àà [0, ùêø‚àí1] is represented with a 4-dimensional embedding ùëí<sub>ùë§</sub> vector. Setting <i>i</i> ‚àà [0, 2), then, for even indices  of 4-dimensional embedding vector, we will use sin(pos/10000<i><sup>2i/d<sub>model</sub></sup></i>) function, whereas for odd indices, we will use cos(pos/10000<i><sup>2i/d<sub>model</sub></sup></i>). \n",
    "\n",
    "Let's call our embedding vector index <i>k</i>, where <i>k</i> ‚àà [0, 2<i>i</i>). The first position in our input sentence is pos = 0, and first index of the embedding vector is <i>k</i>=0. Now, the first PE (positional encoding) for the first dimension, <i>k</i> = 0, of the embedding vector will be sin(0/10000<sup>0/4</sup>), and the second PE for the second dimension, k = 1, will be cos(0/10000<sup>0/4</sup>). For the third, k = 2, and fourth dimensions, k =3, the PEs will be sin(0/10000<sup>2/4</sup>) and cos(0/10000<sup>2/4</sup>), respectively. \n",
    "\n",
    "Now we can write down the positional encoding for the first word of the input sequence:\n",
    "\n",
    "PE (pos =0) = [sin(0/10000<sup>0/4</sup>), cos(0/10000<sup>0/4</sup>), sin(0/10000<sup>2/4</sup>), cos(0/10000<sup>2/4</sup>)].\n",
    "\n",
    "in simple form:\n",
    "\n",
    "PE (pos =0) = [sin(0/10000<sup>0</sup>), cos(0/10000<sup>0</sup>), sin(0/100), cos(0/100)] = [0, 1, 0, 1].\n",
    "\n",
    "The next step is to add this vector to the embedding vector, e<sub>ùë§</sub>, and obtain a new vector, e'<sub>ùë§</sub>:\n",
    "\n",
    "e'<sub>ùë§</sub> = PE (pos =0) + e<sub>ùë§</sub>.\n",
    "\n",
    "And we calculate e'<sub>ùë§</sub> for each word in the input sequence, at pos = 1, 2, ‚Ä¶ L - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "186px",
    "left": "619px",
    "top": "238px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
