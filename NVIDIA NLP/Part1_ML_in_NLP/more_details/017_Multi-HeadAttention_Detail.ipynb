{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are two heads better than one?  How about eight?  A refinement of self-attention is called “multi-headed” attention, which allows the model to focus on different positions or sub-spaces. \n",
    "\n",
    "There are h = 8 parallel attention layers, or heads, in the Transformer architecture. This means that there are eight version of self-attention, all running simultaneously.\n",
    "\n",
    "<center><figure>\n",
    "    <img src=\"../images/multiheadattention.png\" width=\"300\">\n",
    "    <figcaption>Figure 7. Multi-Head Attention</figcaption>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/multiheadattention1.png\" width=\"600\"> \n",
    "where $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$, and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention is essentially attention repeated several times in parallel. If we do the same self-attention calculation outlined above, h = 8 different times with different weight matrices, we end up with eight different <b>Z</b> matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><figure>\n",
    "    <img src=\"../images/multihead.png\">\n",
    "    <figcaption>Figure 8. Multi-Head Attention Mechanism</figcaption>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a single attention head has a simple structure: it applies a unique linear transformation to its input queries, keys, and values, computes the attention score between each query and key, then uses it to weight the values and sum them up. The multi-head attention block just applies multiple blocks in parallel, concatenates their outputs, then applies one single linear transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a residual connection is employed around each of the two sub-layers, followed by layer normalization. In mathematical terms, that is, the output of each sub-layer\n",
    "is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. \n",
    "\n",
    "Steps to add residual connection, <i><b>X<sup>'</sup></b></i>, to the output of the multi-head attention layer, and then apply layer normalization are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate positional embeddings for the input matrix.\n",
    "\n",
    "<b>X′</b> = PE(<b>X</b>) +  <b>X</b> where <b>X′</b> ∈ℝ<sup>n<sub>input</sub>×d<sub>model</sub></sup>.\n",
    "\n",
    "2. Perform multi-head attention layer, and produce the output matrix <b>Z<sup>E</sup><sub>1,1</sub></b>.\n",
    "\n",
    "MultiHead(Q, K, V ) = Concat(head<sub>1</sub>, ..., head<sub>h</sub>)W<sup>O</sup>\n",
    "\n",
    "3. Use residual connection and apply layer normalization to obtain <b>Z<sup>E</sup><sub>1,2</sub></b>  ∈ℝ<sup>n<sub>input</sub>×d<sub>model</sub></sup>.\n",
    "\n",
    "<b>Z<sup>E</sup><sub>1,2</sub></b> = LayerNorm(<b>X′</b> + <b>Z<sup>E</sup><sub>1,1</sub></b>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a summary the left side of the Figure 1 would work like this:\n",
    "     \n",
    "Step1_out = Embedding512 + PositionEncoding512\n",
    "\n",
    "Step2_out = layer_normalization(multihead_attention(Step1_out) + Step1_out)\n",
    "\n",
    "Step3_out = layer_normalization(FFN(Step2_out) + Step2_out)\n",
    "\n",
    "out_enc = Step3_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"../images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "186px",
    "left": "619px",
    "top": "238px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
