{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents from First Principles\n",
    "> Following along Pramod Goyal's excellent blog [here](https://goyalpramod.github.io/blogs/AI_agents_from_first_principles/). I'll be using OpenRouter for API calls. This file should serve as a decent mostly-code reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "This has a needlessly complex name - **tools are just functions.**\n",
    "\n",
    "Yes, that’s it - they are functions with defined inputs and outputs. These functions are provided to an LLM as a schema, and the model extracts input values from user queries to call these functions.\n",
    "\n",
    "Here's a sample tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city: str, date: str | None = None) -> dict[str, int | str]:\n",
    "    \"\"\"\n",
    "    Gets weather information for a specific city and date.#-\n",
    "\n",
    "    Args:\n",
    "        city: Name of the city#-\n",
    "        date: Date in YYYY-MM-DD format, defaults to today#-\n",
    "\n",
    "    Returns:\n",
    "        dict: Weather information including temperature and conditions#-\n",
    "    \"\"\"\n",
    "    # Implementation here\n",
    "    return {\"temperature\": 25, \"conditions\": \"sunny\", \"humidity\": 60}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Callable[..., ...]` is not allowed it seems, you have to use `Callable[..., Any]`\n",
    "\n",
    "In many libraries you will find them using `@tool` on top of functions. This is **just a Python decorator that adds metadata.** Let’s create a simple one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is tool: True\n",
      "Description: Calculate area of a rectangle.\n",
      "Parameters: OrderedDict([('length', <Parameter \"length: float\">), ('width', <Parameter \"width: float\">)])\n",
      "Result: 15\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "from typing import Callable, Any, Dict\n",
    "import inspect\n",
    "\n",
    "\n",
    "# This is the decorator that we define\n",
    "def tool(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    Decorator that converts a function into an LLM tool by adding metadata.\n",
    "\n",
    "    Args:\n",
    "        func Callable[..., Any]: Function to convert into a tool\n",
    "\n",
    "    Returns:\n",
    "        Callable[..., Any]: Decorated function with metadata\n",
    "    \"\"\"\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    # Add function metadata\n",
    "    wrapper.is_tool: bool = True  # type: ignore\n",
    "    wrapper.description: str = func.__doc__ if func.__doc__ else \"\"  # type: ignore\n",
    "    wrapper.parameters: inspect.Signature = inspect.signature(func).parameters  # type: ignore\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Usage example\n",
    "@tool\n",
    "def calculate_area(length: float, width: float) -> float:\n",
    "    \"\"\"Calculate area of a rectangle.\"\"\"\n",
    "    return length * width\n",
    "\n",
    "\n",
    "# Let's see what the decorator added\n",
    "print(\n",
    "    \"Is tool:\",\n",
    "    calculate_area.is_tool,  # type: ignore\n",
    ")  # Is tool: True\n",
    "\n",
    "print(\n",
    "    \"Description:\",\n",
    "    calculate_area.description,  # type: ignore\n",
    ")  # Description: Calculate area of a rectangle.\n",
    "\n",
    "print(\n",
    "    \"Parameters:\",\n",
    "    calculate_area.parameters,  # type: ignore\n",
    ")  # Parameters: <Signature (length: float, width: float) -> float>\n",
    "print(\"Result:\", calculate_area(5, 3))  # result: 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **decorator doesn’t modify how the function works** - it simply passes through all calls to the original function while **adding extra attributes**. These attributes (`is_tool`, `description`, `parameters`) help the LLM understand:\n",
    "\n",
    "- That this function is available as a tool\n",
    "- What the function does (from the docstring)\n",
    "- What parameters it expects (from the signature)\n",
    "\n",
    "This metadata is then used to create the function schema that we send to the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Memory in LLM agents can be handled in two ways - through context window or external databases. Here is a basic example -\n",
    "\n",
    "```python\n",
    "import datetime\n",
    "\n",
    "class AgentMemory:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize memory storage\"\"\"\n",
    "        self.conversations = []  # Short-term memory\n",
    "        self.db = Database()   # Long-term memory\n",
    "\n",
    "    def add_to_memory(self, message: str, role: str):\n",
    "        \"\"\"Add a new message to memory\"\"\"\n",
    "        # Keep last N messages in context\n",
    "        self.conversations.append(\n",
    "            {\"role\": role, \"content\": message, \"timestamp\": datetime.now()}\n",
    "        )\n",
    "\n",
    "        # Store in long-term memory\n",
    "        self.db.store(message, role)\n",
    "\n",
    "    def get_relevant_context(self, query: str) -> List[str]:\n",
    "        \"\"\"Retrieve relevant information from long-term memory\"\"\"\n",
    "        return self.db.search(query)\n",
    "\n",
    "    def get_context_window(self) -> List[dict]:\n",
    "        \"\"\"Get recent conversations for context window\"\"\"\n",
    "        return self.conversations[-10:]  # Last 10 messages\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here’s a basic RAG implementation:**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from qdrant_client import QdrantClient \n",
    "from typing import List, Dict\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize embedding model\n",
    "        self.embedder = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.qdrant: QdrantClient = QdrantClient(\"localhost\", port=6333)\n",
    "        \n",
    "    def add_documents(self, documents: List[str]):\n",
    "        # Create embeddings\n",
    "        embeddings = self.embedder.encode(documents)\n",
    "        \n",
    "        # Store in vector DB\n",
    "        self.qdrant.upload_collection(\n",
    "            collection_name=\"knowledge_base\",\n",
    "            vectors=embeddings,\n",
    "            payload=documents\n",
    "        )\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        # Get query embedding\n",
    "        query_embedding = self.embedder.encode(query)\n",
    "        \n",
    "        # Search similar documents\n",
    "        results = self.qdrant.search(\n",
    "            collection_name=\"knowledge_base\",\n",
    "            query_vector=query_embedding,\n",
    "            limit=k\n",
    "        )\n",
    "        return [hit.payload for hit in results]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Document Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into smaller chunks while preserving sentence boundaries\n",
    "    \"\"\"\n",
    "    sentences = text.split(\". \")\n",
    "    chunks: List[str] = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence) > chunk_size:\n",
    "            # if new is bigger, then add existing to chunks. Then make new as current\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = len(sentence)\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += len(sentence)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardize text format and remove noise\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s.,!?]\", \"\", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'This is a long sentence',\n",
       " 'It should be split into smaller chunks',\n",
       " 'My already existing persona needs slight modification such that there should me higher interest in tyres and robotics']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_text(\n",
    "    \"This is a long sentence. It should be split into smaller chunks. My already existing persona needs slight modification such that there should me higher interest in tyres and robotics. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for AI Agents\n",
    "\n",
    "### Core Guidelines\n",
    "As pointed out in the [smolagents guide](https://huggingface.co/docs/smolagents/tutorials/building_good_agents), minimize LLM calls by:\n",
    "\n",
    "- Combining related tools into single functions\n",
    "- Using deterministic logic over LLM-based decisions\n",
    "- Caching responses for similar queries\n",
    "\n",
    "\n",
    "```python\n",
    "# Bad: Multiple LLM calls\n",
    "def process_order(order_details):\n",
    "    # First LLM call to validate\n",
    "    validated = llm.validate(order_details)\n",
    "    if not validated:\n",
    "        return \"Invalid order\"\n",
    "        \n",
    "    # Second LLM call to format\n",
    "    formatted = llm.format(order_details)\n",
    "    \n",
    "    # Third LLM call to process\n",
    "    return llm.process(formatted)\n",
    "\n",
    "# Good: Single LLM call\n",
    "def process_order(order_details):\n",
    "    # Validate with regular code\n",
    "    if not is_valid_order(order_details):\n",
    "        return \"Invalid order\"\n",
    "        \n",
    "    # Format with template\n",
    "    formatted = ORDER_TEMPLATE.format(**order_details)\n",
    "    \n",
    "    # Single LLM call\n",
    "    return llm.process(formatted)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Pitfalls\n",
    "\n",
    "**Over-relying on LLMs**\n",
    "\n",
    "Bad:\n",
    "\n",
    "```python\n",
    "def validate_email(email):\n",
    "    response = llm.call(\"Is this a valid email: \" + email)\n",
    "    return \"valid\" in response.lower()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Good:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "def validate_email(email):\n",
    "    pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Bloat**\n",
    "\n",
    "Bad:\n",
    "\n",
    "```python\n",
    "system_message = \"\"\"You are an AI assistant that helps with tasks.\n",
    "You should be helpful, concise, and clear.\n",
    "Always format responses properly.\n",
    "Remember to be polite.\n",
    "Double check your answers.\n",
    "Consider edge cases.\n",
    "...50 more lines of instructions...\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Good:\n",
    "\n",
    "```python\n",
    "system_message = \"\"\"You are a task-focused AI assistant.\n",
    "Format: Generate concise, actionable responses.\n",
    "Priority: Accuracy and clarity.\"\"\"\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool Proliferation**\n",
    "\n",
    "Instead of:\n",
    "\n",
    "```py\n",
    "tools = [\n",
    "    fetch_weather,\n",
    "    fetch_temperature,\n",
    "    fetch_humidity,\n",
    "    fetch_wind_speed,\n",
    "    fetch_precipitation\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Do this:\n",
    "\n",
    "```py\n",
    "tools = [\n",
    "    fetch_weather_data  # Returns complete weather info in one call\n",
    "]\n",
    "\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Framework**\n",
    "\n",
    "```py\n",
    "class AgentTester:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.metrics = {\n",
    "            'success_rate': 0,\n",
    "            'response_time': [],\n",
    "            'token_usage': [],\n",
    "            'error_rate': 0\n",
    "        }\n",
    "    \n",
    "    def test_case(self, input_query, expected_output):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = self.agent.run(input_query)\n",
    "            success = self.validate_response(response, expected_output)\n",
    "            self.metrics['success_rate'] += success\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics['error_rate'] += 1\n",
    "            \n",
    "        self.metrics['response_time'].append(time.time() - start_time)\n",
    "        \n",
    "    def validate_response(self, response, expected):\n",
    "        # Implement validation logic\n",
    "        pass\n",
    "        \n",
    "    def get_metrics_report(self):\n",
    "        return {\n",
    "            'avg_response_time': np.mean(self.metrics['response_time']),\n",
    "            'success_rate': self.metrics['success_rate'],\n",
    "            'error_rate': self.metrics['error_rate']\n",
    "        }\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Agent\n",
    "<img src=\"readme-images/1.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (as a large language model, I don’t have a name.) You can just call me Bard. 😊 \n",
      "\n",
      "It’s nice to meet you!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=openrouter_api_key,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-3-1b-it:free\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is your name?\"},\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM call\n",
    "\n",
    "> I tried to use the free `\"google/gemma-3-1b-it:free\"`, but it does not have tool calling abilities it seems. `openai/gpt-3.5-turbo-0613` does not support tool calling as well. `o3-mini` and `4o` models seem to perform the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from openai.types.chat.chat_completion_message import ChatCompletionMessage\n",
    "\n",
    "\n",
    "def run_llm(\n",
    "    content: Optional[str] = None,\n",
    "    messages: Optional[List[str]] = [],\n",
    "    # tool_schemas: Optional[List[str]] = [],#-\n",
    "    tool_schemas: Optional[List[Dict[str, Any]]] = [],\n",
    "    system_message: str = \"You are a helpful assistant.\",\n",
    ") -> List[ChatCompletionMessage]:\n",
    "    # Build base request parameters\n",
    "    request_params: Dict[str, Any] = {\n",
    "        \"model\": \"openai/o3-mini\",  # \"openai/gpt-4o-2024-11-20\",  # openai/gpt-3.5-turbo-0613\", # ,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ]\n",
    "        + messages,  # type: ignore\n",
    "    }\n",
    "\n",
    "    # Only add tools parameter if tool_schemas is provided and non-empty\n",
    "    if tool_schemas:\n",
    "        request_params[\"tools\"] = tool_schemas\n",
    "\n",
    "    # Make the API call with conditional parameters\n",
    "    completion: ChatCompletion = client.chat.completions.create(**request_params)  # type: ignore\n",
    "\n",
    "    response: Dict[str, Any] = completion.choices[0].message  # type: ignore\n",
    "    messages.append(response)  # type: ignore\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "# {\"conversationId\":\"cf746cdd-b8fc-498a-b3e2-fd84a89c4cbf\",\"source\":\"instruct\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let’s give it the same prompt that we made earlier and see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessage(content='[42, 2, 13]', role='assistant', function_call=None, tool_calls=None, refusal=None)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_llm(\n",
    "    content=\"\"\"[\"apple\", \"pie\", 42, 2, 13]\"\"\",\n",
    "    system_message=\"\"\"\n",
    "    You are an expert classifier, which classifies strings and integers.\\\n",
    "    Given a list of numbers and words, only return the numbers as a list.\\\n",
    "    You will be given the inputs inside <input> tags.\\\n",
    "\n",
    "    Input:  <input>[\"hello\", 42, \"pizza\", 2, 5]</input>\n",
    "    Output: [42,2,5]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# [ChatCompletionMessage(content='[42, 2, 13]', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works as expected, Let’s build on top of this by giving our LLM the ability to calculate sums of numbers. (You can define the function anyhow you would like)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM call + Tools\n",
    "\n",
    "**Let’s create a utility function that takes another function and creates it’s schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_schema(func: Callable[..., Any]) -> Dict[str, Any]:\n",
    "    type_map: Dict[Any, str] = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get the function's signature\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    # Define the parameters dictionary\n",
    "    parameters: Dict[str, str | Dict[str, str]] = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    # No default value means required\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty  # type: ignore\n",
    "    ]\n",
    "\n",
    "    # Return the schema dictionary\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A function that takes another function can be simplified using a decorator. That is what one usually sees in most libraries/framework “@tool”. Now let’s create a sum tool.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"function\",\n",
      "  \"function\": {\n",
      "    \"name\": \"add_numbers\",\n",
      "    \"description\": \"This function takes a List of numbers as Input and returns the sum\\n\\n    Args:\\n        input: List[int]\\n        output: int\",\n",
      "    \"parameters\": {\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"num_list\": {\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"num_list\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def add_numbers(num_list: List[int]):\n",
    "    \"\"\"\n",
    "    This function takes a List of numbers as Input and returns the sum\n",
    "\n",
    "    Args:\n",
    "        input: List[int]\n",
    "        output: int\n",
    "    \"\"\"\n",
    "    return sum(num_list)\n",
    "\n",
    "\n",
    "schema = function_to_schema(add_numbers)\n",
    "print(json.dumps(schema, indent=2))\n",
    "\n",
    "# {\n",
    "#   \"type\": \"function\",\n",
    "#   \"function\": {\n",
    "#     \"name\": \"add_numbers\",\n",
    "#     \"description\": \"This function takes a List of numbers as Input and returns the sum \\n  \\n  Args:\\n      input: List[int]\\n      output: int\",\n",
    "#     \"parameters\": {\n",
    "#       \"type\": \"object\",\n",
    "#       \"properties\": {\n",
    "#         \"num_list\": {\n",
    "#           \"type\": \"string\"\n",
    "#         }\n",
    "#       },\n",
    "#       \"required\": [\n",
    "#         \"num_list\"\n",
    "#       ]\n",
    "#     }\n",
    "#   }\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More dummy proof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import ast\n",
    "\n",
    "\n",
    "def add_numbers(num_list: Union[List[int], str]) -> int:\n",
    "    \"\"\"\n",
    "    This function takes either a List of integers or a string representation of a list\n",
    "    and returns the sum of the numbers.\n",
    "\n",
    "    Args:\n",
    "        num_list: List[int] or str - Either a list of integers or a string representing a list\n",
    "            e.g. \"[1, 2, 3]\" or [1, 2, 3]\n",
    "\n",
    "    Returns:\n",
    "        int: The sum of all numbers in the list\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the string cannot be converted to a list of integers\n",
    "        SyntaxError: If the string is not properly formatted\n",
    "    \"\"\"\n",
    "    if isinstance(num_list, str):\n",
    "        try:\n",
    "            num_list = ast.literal_eval(num_list)\n",
    "            if not isinstance(num_list, list):\n",
    "                raise ValueError(\"String must represent a list\")\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            raise ValueError(f\"Invalid input string format: {e}\")\n",
    "\n",
    "    # Verify all elements are integers\n",
    "    if not all(isinstance(x, int) for x in num_list):  # type: ignore\n",
    "        raise ValueError(\"All elements must be integers\")\n",
    "\n",
    "    return sum(num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create an additional `multiply_numbers` tool too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "# import ast # it is already built in Python it seems\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "\n",
    "def multiply_numbers(num_list: Union[List[int], str]) -> int:\n",
    "    \"\"\"\n",
    "    This function takes either a List of integers or a string representation of a list\n",
    "    and returns the product of all numbers.\n",
    "\n",
    "    Args:\n",
    "        num_list: List[int] or str - Either a list of integers or a string representing a list\n",
    "            e.g. \"[1, 2, 3]\" or [1, 2, 3]\n",
    "\n",
    "    Returns:\n",
    "        int: The product of all numbers in the list\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the string cannot be converted to a list of integers,\n",
    "                   if the list is empty, or if any element is not an integer\n",
    "        SyntaxError: If the string is not properly formatted\n",
    "    \"\"\"\n",
    "    # Handle string input\n",
    "    if isinstance(num_list, str):\n",
    "        try:\n",
    "            num_list = ast.literal_eval(num_list)\n",
    "            if not isinstance(num_list, list):\n",
    "                raise ValueError(\"String must represent a list\")\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            raise ValueError(f\"Invalid input string format: {e}\")\n",
    "\n",
    "    # Check if list is empty\n",
    "    if not num_list:\n",
    "        raise ValueError(\"List cannot be empty\")\n",
    "\n",
    "    # Verify all elements are integers\n",
    "    if not all(isinstance(x, int) for x in num_list):  # type: ignore\n",
    "        raise ValueError(\"All elements must be integers\")\n",
    "\n",
    "    # Calculate product using reduce and multiplication operator\n",
    "    return reduce(mul, num_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to use this tool with our LLM to see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessage(content='[42, 2, 13]', role='assistant', function_call=None, tool_calls=None, refusal=None), ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_fgOnwEuDGdDzWkg819ZQqb2K', function=Function(arguments='{\"num_list\": \"[23,51,321]\"}', name='add_numbers'), type='function', index=0)], refusal=None), ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yMVX4IkdRk6ewAGcrohVKhEB', function=Function(arguments='{\"num_list\": \"[23,51,321]\"}', name='add_numbers'), type='function', index=0)], refusal=None), ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Fz1WeGeO38Psqp7DeuCC8ueV', function=Function(arguments='{\"num_list\": \"[23,51,321]\"}', name='add_numbers'), type='function', index=0)], refusal=None)]\n"
     ]
    }
   ],
   "source": [
    "tools = [add_numbers, multiply_numbers]\n",
    "tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "\n",
    "response = run_llm(\n",
    "    content=\"\"\"\n",
    "    [23,51,321]\n",
    "    \"\"\",\n",
    "    system_message=\"\"\"\n",
    "    Use the appropriate tool to calculate the sum of numbers, and only the tool and nothing else.\n",
    "    \"\"\",\n",
    "    tool_schemas=tool_schemas,\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "# [ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_enJSWBrayTgSlFCKgrgw6BGz', function=Function(arguments='{\"num_list\":\"[23,51,321]\"}', name='add_numbers'), type='function')])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessage(content='[42, 2, 13]', role='assistant', function_call=None, tool_calls=None, refusal=None),\n",
       " ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_fgOnwEuDGdDzWkg819ZQqb2K', function=Function(arguments='{\"num_list\": \"[23,51,321]\"}', name='add_numbers'), type='function', index=0)], refusal=None),\n",
       " ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yMVX4IkdRk6ewAGcrohVKhEB', function=Function(arguments='{\"num_list\": \"[23,51,321]\"}', name='add_numbers'), type='function', index=0)], refusal=None),\n",
       " ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Fz1WeGeO38Psqp7DeuCC8ueV', function=Function(arguments='{\"num_list\": \"[23,51,321]\"}', name='add_numbers'), type='function', index=0)], refusal=None)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Now that we can get the tool names and arguments, its time to create another utility function that can take these info and ACTUALLY EXECUTE THEM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion_message_tool_call import (\n",
    "    ChatCompletionMessageToolCall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " function,\n",
       " <function __main__.add_numbers(num_list: Union[List[int], str]) -> int>,\n",
       " <function __main__.multiply_numbers(num_list: Union[List[int], str]) -> int>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tools), type(tools[1]), tools[0], tools[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: add_numbers({'num_list': '[23,51,321]'})\n"
     ]
    }
   ],
   "source": [
    "tools_map = {tool.__name__: tool for tool in tools}\n",
    "messages: List[Dict[str, Any]] = []\n",
    "\n",
    "\n",
    "def execute_tool_call(\n",
    "    tool_call: ChatCompletionMessageToolCall, tools_map: Dict[str, Callable[..., int]]\n",
    ") -> int:\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"Assistant: {name}({args})\")\n",
    "\n",
    "    # call corresponding function with provided arguments\n",
    "    return tools_map[name](**args)\n",
    "\n",
    "\n",
    "# NOTE: Had to shift from response[0] to response[1]\n",
    "for tool_call in response[1].tool_calls:  # type:ignore\n",
    "    result = execute_tool_call(tool_call, tools_map)\n",
    "\n",
    "    # add result back to conversation\n",
    "    result_message: Dict[str, Any] = {\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"content\": result,\n",
    "    }\n",
    "    messages.append(result_message)\n",
    "\n",
    "# Assistant: add_numbers({'num_list': '[23,51,321]'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we would like our llms to take this response and send an output to the user. Let’s do that.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(\n",
    "    system_message: str, tools: Callable[..., int], messages: List[Dict[str, str]]\n",
    "):\n",
    "    num_init_messages = len(messages)\n",
    "    messages = messages.copy()\n",
    "\n",
    "    while True:\n",
    "        # turn python functions into tools and save a reverse map\n",
    "        tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "        tools_map = {tool.__name__: tool for tool in tools}\n",
    "\n",
    "        # === 1. get openai completion ===\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_message}] + messages,\n",
    "            tools=tool_schemas or None,\n",
    "        )\n",
    "        message = response.choices[0].message\n",
    "        messages.append(message)\n",
    "\n",
    "        if message.content:  # print assistant response\n",
    "            print(\"Assistant:\", message.content)\n",
    "\n",
    "        if not message.tool_calls:  # if finished handling tool calls, break\n",
    "            break\n",
    "\n",
    "        # === 2. handle tool calls ===\n",
    "\n",
    "        for tool_call in message.tool_calls:\n",
    "            result = execute_tool_call(tool_call, tools_map)\n",
    "\n",
    "            result_message = {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": str(result),\n",
    "            }\n",
    "            messages.append(result_message)\n",
    "\n",
    "    # ==== 3. return new messages =====\n",
    "    return messages[num_init_messages:]\n",
    "\n",
    "\n",
    "def execute_tool_call(tool_call, tools_map):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"Assistant: {name}({args})\")\n",
    "\n",
    "    # call corresponding function with provided arguments\n",
    "    return str(tools_map[name](**args))\n",
    "\n",
    "\n",
    "tools = [add_numbers]\n",
    "messages = []\n",
    "system_message = \"\"\"\n",
    "    You are an expert number processor and classifier. Your task is to extract and sum only the numbers from any input, ignoring all non-numeric values.\n",
    "\n",
    "    Rules:\n",
    "    1. Only process numeric values (integers)\n",
    "    2. Ignore all non-numeric values (strings, letters, special characters)\n",
    "    3. Use the add_numbers function to calculate the sum\n",
    "    4. Format the input properly before passing to add_numbers\n",
    "\n",
    "    Examples:\n",
    "    Input: <input>[\"hello\", 42, \"pizza\", 2, 5]</input>\n",
    "    Process: Extract numbers [42, 2, 5]\n",
    "    Output: 49\n",
    "\n",
    "    Input: <input>[asj,cg,111,42,2]</input>\n",
    "    Process: Extract numbers [111, 42, 2]\n",
    "    Output: 155\n",
    "\n",
    "    Input: <input>[text, more, 100, words, 50]</input>\n",
    "    Process: Extract numbers [100, 50]\n",
    "    Output: 150\n",
    "\n",
    "    For any input, first extract the numbers, then use add_numbers function to calculate their sum.\n",
    "    Make sure to format the input as a proper list string with square brackets before passing to add_numbers.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "while True:\n",
    "    user = input(\"User: \")\n",
    "    messages.append({\"role\": \"user\", \"content\": user})\n",
    "\n",
    "    new_messages = run_agent(system_message, tools, messages)\n",
    "    messages.extend(new_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Agent(LLM call + Tools + Pydantic Model)\n",
    "\n",
    "Let’s first build a model (This is the pydantic model, I will be referring to these as models. And Large Language Models as LLMs)\n",
    "\n",
    "```\n",
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    llm: str = \"gpt-4o-mini\"\n",
    "    system_message: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Now we can modify the code we wrote earlier to use this model\n",
    "\n",
    "```\n",
    "def run_agent(agent, messages):\n",
    "\n",
    "    num_init_messages = len(messages)\n",
    "    messages = messages.copy()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # turn python functions into tools and save a reverse map\n",
    "        tool_schemas = [function_to_schema(tool) for tool in agent.tools]\n",
    "        tools_map = {tool.__name__: tool for tool in agent.tools}\n",
    "\n",
    "        # === 1. get openai completion ===\n",
    "        response = client.chat.completions.create(\n",
    "            model=agent.llm,\n",
    "            messages=[{\"role\": \"system\", \"content\": agent.system_message}] + messages,\n",
    "            tools=tool_schemas or None,\n",
    "        )\n",
    "        message = response.choices[0].message\n",
    "        messages.append(message)\n",
    "\n",
    "        if message.content:  # print assistant response\n",
    "            print(\"Assistant:\", message.content)\n",
    "\n",
    "        if not message.tool_calls:  # if finished handling tool calls, break\n",
    "            break\n",
    "\n",
    "        # === 2. handle tool calls ===\n",
    "\n",
    "        for tool_call in message.tool_calls:\n",
    "            result = execute_tool_call(tool_call, tools_map)\n",
    "\n",
    "            result_message = {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": str(result),\n",
    "            }\n",
    "            messages.append(result_message)\n",
    "\n",
    "    # ==== 3. return new messages =====\n",
    "    return messages[num_init_messages:]\n",
    "\n",
    "\n",
    "def execute_tool_call(tool_call, tools_map):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"Assistant: {name}({args})\")\n",
    "\n",
    "    # call corresponding function with provided arguments\n",
    "    return str(tools_map[name](**args))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "And just as easily we can run multiple agents\n",
    "\n",
    "```\n",
    "calculator_add = Agent(\n",
    "    name=\"Addition Calculator\",\n",
    "    system_message=\"\"\"You are an expert number processor. Extract and sum only the numbers from any input, ignoring non-numeric values.\n",
    "    Example:\n",
    "    Input: [text, 100, words, 50]\n",
    "    Process: Extract numbers [100, 50]\n",
    "    Output: 150\"\"\",\n",
    "    tools=[add_numbers],\n",
    ")\n",
    "\n",
    "calculator_multiply = Agent(\n",
    "    name=\"Multiplication Calculator\",\n",
    "    system_message=\"\"\"You are an expert number processor. Extract and multiply only the numbers from any input, ignoring non-numeric values.\n",
    "    Example:\n",
    "    Input: [text, 4, words, 5]\n",
    "    Process: Extract numbers [4, 5]\n",
    "    Output: 20\"\"\",\n",
    "    tools=[multiply_numbers],\n",
    ")\n",
    "\n",
    "messages = []\n",
    "user_query = \"[hello, 10, world, 5, test, 2]\"\n",
    "print(\"User:\", user_query)\n",
    "messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "response = run_agent(calculator_add, messages)  # Addition calculator\n",
    "messages.extend(response)\n",
    "\n",
    "user_query = \"Now multiply these numbers\"  # implicitly refers to the numbers from previous input\n",
    "print(\"User:\", user_query)\n",
    "messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "response = run_agent(calculator_multiply, messages)  # Multiplication calculator\n",
    "\n",
    "# User: [hello, 10, world, 5, test, 2]\n",
    "# Assistant: add_numbers({'num_list': '[10, 5, 2]'})\n",
    "# Assistant: The sum of the numbers extracted from the input is 17.\n",
    "# User: Now multiply these numbers\n",
    "# Assistant: multiply_numbers({'num_list': '[10, 5, 2]'})\n",
    "# Assistant: The product of the numbers extracted from the input is 100.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "When building multi-agent systems, we want seamless transitions between agents based on the task requirements. The naive approach would be manual handoffs, but we can make this smarter.\n",
    "\n",
    "Instead of manually switching agents, we can create a handoff mechanism where agents decide themselves when to transfer control. Let’s build this:\n",
    "\n",
    "```\n",
    "class Response(BaseModel):\n",
    "    agent: Optional[Agent]\n",
    "    messages: list\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "def run_agent(agent, messages):\n",
    "\n",
    "    current_agent = agent\n",
    "    num_init_messages = len(messages)\n",
    "    messages = messages.copy()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # turn python functions into tools and save a reverse map\n",
    "        tool_schemas = [function_to_schema(tool) for tool in current_agent.tools]\n",
    "        tools = {tool.__name__: tool for tool in current_agent.tools}\n",
    "\n",
    "        # === 1. get openai completion ===\n",
    "        response = client.chat.completions.create(\n",
    "            model=agent.llm,\n",
    "            messages=[{\"role\": \"system\", \"content\": current_agent.system_message}]\n",
    "            + messages,\n",
    "            tools=tool_schemas or None,\n",
    "        )\n",
    "        message = response.choices[0].message\n",
    "        messages.append(message)\n",
    "\n",
    "        if message.content:  # print agent response\n",
    "            print(f\"{current_agent.name}:\", message.content)\n",
    "\n",
    "        if not message.tool_calls:  # if finished handling tool calls, break\n",
    "            break\n",
    "\n",
    "        # === 2. handle tool calls ===\n",
    "\n",
    "        for tool_call in message.tool_calls:\n",
    "            result = execute_tool_call(tool_call, tools, current_agent.name)\n",
    "\n",
    "            if type(result) is Agent:  # if agent transfer, update current agent\n",
    "                current_agent = result\n",
    "                result = (\n",
    "                    f\"Transfered to {current_agent.name}. Adopt persona immediately.\"\n",
    "                )\n",
    "\n",
    "            result_message = {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": str(result),\n",
    "            }\n",
    "            messages.append(result_message)\n",
    "\n",
    "    # ==== 3. return last agent used and new messages =====\n",
    "    return Response(agent=current_agent, messages=messages[num_init_messages:])\n",
    "\n",
    "\n",
    "def execute_tool_call(tool_call, tools, agent_name):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    return str(tools[name](**args))  # call corresponding function with provided arguments\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Popular Agentic systems\n",
    "-----------------------\n",
    "\n",
    "The above is all the knowledge you require to build more complex agentic systems, let’s explore a few of them. Solely using what we have learned so far.\n",
    "\n",
    "Langgraph has a nice list of agentic systems in my opinion, you can check them out [here](https://langchain-ai.github.io/langgraph/tutorials/#multi-agent-systems).\n",
    "\n",
    "### ReAct\n",
    "\n",
    "![Image of super special artist](https://goyalpramod.github.io/assets/blog_assets/ai_agents/4.webp)\n",
    "\n",
    "> Image taken from the [ReAct paper](https://arxiv.org/abs/2210.03629)\n",
    "\n",
    "```\n",
    "def add_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Add two numbers together.\n",
    "\n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "\n",
    "    Returns:\n",
    "        The sum of a and b\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def subtract_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Subtract b from a.\n",
    "\n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "\n",
    "    Returns:\n",
    "        The result of a - b\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "def multiply_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Multiply two numbers together.\n",
    "\n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "\n",
    "    Returns:\n",
    "        The product of a and b\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def divide_numbers(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Divide a by b.\n",
    "\n",
    "    Args:\n",
    "        a: First number (dividend)\n",
    "        b: Second number (divisor)\n",
    "\n",
    "    Returns:\n",
    "        The result of a / b\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If b is zero\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "    return a / b\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "react_system_message = \"\"\"You are a mathematical reasoning agent that follows the ReAct pattern: Thought, Action, Observation.\n",
    "\n",
    "For each step of your reasoning:\n",
    "1. Thought: First explain your thinking\n",
    "2. Action: Then choose and use a tool\n",
    "3. Observation: Observe the result\n",
    "\n",
    "Format your responses as:\n",
    "Thought: <your reasoning>\n",
    "Action: <tool_name>(<parameters>)\n",
    "Observation: <result>\n",
    "Thought: <your next step>\n",
    "...\n",
    "\n",
    "Available tools:\n",
    "- add_numbers(a, b): Add two numbers\n",
    "- subtract_numbers(a, b): Subtract b from a\n",
    "- multiply_numbers(a, b): Multiply two numbers\n",
    "- divide_numbers(a, b): Divide a by b\n",
    "\n",
    "Always break down complex calculations into steps using this format.\"\"\"\n",
    "\n",
    "react_agent = Agent(\n",
    "    name=\"ReActMath\",\n",
    "    llm=\"gpt-4o-mini\",\n",
    "    system_message=react_system_message,\n",
    "    tools=[add_numbers, subtract_numbers, multiply_numbers, divide_numbers]\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "# Example usage\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Calculate (23 + 7) * 3 - 15\"\n",
    "}]\n",
    "\n",
    "response = run_agent(react_agent, messages)\n",
    "\n",
    "# ReActMath: Thought: First, I need to calculate the sum of 23 and 7. Then I will multiply the result by 3, and finally, I will subtract 15 from that product. I'll break this down into steps for clarity.\n",
    "\n",
    "# Action: I will first add 23 and 7.\n",
    "# functions.add_numbers({ a: 23, b: 7 })\n",
    "\n",
    "# Observation: Let's perform the addition.\n",
    "# ReActMath: add_numbers({'a': 23, 'b': 7})\n",
    "# ReActMath: Thought: The sum of 23 and 7 is 30. Now, I will multiply this result by 3.\n",
    "\n",
    "# Action: I will multiply 30 by 3.\n",
    "# functions.multiply_numbers({ a: 30, b: 3 })\n",
    "\n",
    "# Observation: Let's perform the multiplication.\n",
    "# ReActMath: multiply_numbers({'a': 30, 'b': 3})\n",
    "# ReActMath: Thought: The product of 30 and 3 is 90. Now, I need to subtract 15 from this result.\n",
    "\n",
    "# Action: I will subtract 15 from 90.\n",
    "# functions.subtract_numbers({ a: 90, b: 15 })\n",
    "\n",
    "# Observation: Let's perform the subtraction.\n",
    "# ReActMath: subtract_numbers({'a': 90, 'b': 15})\n",
    "# ReActMath: Thought: The result of subtracting 15 from 90 is 75. Therefore, the final result of the calculation (23 + 7) * 3 - 15 is 75.\n",
    "\n",
    "# Final Result: 75\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Agentic RAG\n",
    "\n",
    "![Image of super special artist](https://goyalpramod.github.io/assets/blog_assets/ai_agents/3.webp)\n",
    "\n",
    "> Image taken from langgraph [docs](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/)\n",
    "\n",
    "```\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "class DocumentStore:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self.dimension = 384\n",
    "\n",
    "    def add_documents(self, url: str) -> str:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        content = soup.find(id=\"mw-content-text\")\n",
    "        if content:\n",
    "            paragraphs = content.find_all('p')\n",
    "            self.documents = [p.text for p in paragraphs if len(p.text.split()) > 20]\n",
    "\n",
    "        embeddings = self.embedder.encode(self.documents)\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index.add(np.array(embeddings).astype('float32'))\n",
    "        return f\"Processed {len(self.documents)} documents\"\n",
    "\n",
    "    def search(self, query: str, k: int = 3) -> List[str]:\n",
    "        query_vector = self.embedder.encode([query])\n",
    "        D, I = self.index.search(np.array(query_vector).astype('float32'), k)\n",
    "        return [self.documents[i] for i in I[0]]\n",
    "\n",
    "# Initialize document store\n",
    "doc_store = DocumentStore()\n",
    "\n",
    "def retrieve_documents(url: str):\n",
    "    \"\"\"Tool function for document retrieval\"\"\"\n",
    "    return doc_store.add_documents(url)\n",
    "\n",
    "def search_context(query: str):\n",
    "    \"\"\"Tool function for searching documents\"\"\"\n",
    "    return doc_store.search(query)\n",
    "\n",
    "def check_relevance(context: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Tool function to check if retrieved context is relevant using an LLM.\n",
    "\n",
    "    Args:\n",
    "        context: List of context strings to evaluate\n",
    "\n",
    "    Returns:\n",
    "        bool: True if context is relevant, False otherwise\n",
    "    \"\"\"\n",
    "    if not context:\n",
    "        return False\n",
    "\n",
    "    system_message = \"\"\"You are a relevance checking assistant.\n",
    "    Evaluate if the given context is relevant and substantial enough to answer questions.\n",
    "    Return only 'true' or 'false'.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Evaluate if this context is relevant and substantial (contains meaningful information):\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Return only 'true' or 'false'.\"\"\"\n",
    "\n",
    "    messages = run_llm(\n",
    "        content=prompt,\n",
    "        system_message=system_message\n",
    "    )\n",
    "\n",
    "    # Get the last message which contains the LLM's response\n",
    "    result = messages[-1].content.lower().strip()\n",
    "    return result == 'true'\n",
    "\n",
    "def rewrite_query(query: str, context: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Tool function to rewrite a query based on context using an LLM.\n",
    "\n",
    "    Args:\n",
    "        query: Original query to rewrite\n",
    "        context: List of context strings to use for rewriting\n",
    "\n",
    "    Returns:\n",
    "        str: Rewritten query incorporating context\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"You are a query rewriting assistant.\n",
    "    Your task is to rewrite the original query to incorporate relevant context.\n",
    "    Maintain the original intent while making it more specific based on the context.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Original Query: {query}\n",
    "\n",
    "Available Context: {context}\n",
    "\n",
    "Rewrite the query to be more specific using the context.\n",
    "Maintain the original intent but make it more precise.\"\"\"\n",
    "\n",
    "    messages = run_llm(\n",
    "        content=prompt,\n",
    "        system_message=system_message\n",
    "    )\n",
    "\n",
    "    # Get the last message which contains the rewritten query\n",
    "    return messages[-1].content.strip()\n",
    "\n",
    "rag_agent = Agent(\n",
    "    name=\"RAG Agent\",\n",
    "    system_message=\"\"\"You are an intelligent RAG agent that follows a specific workflow:\n",
    "    1. First, determine if you need to retrieve documents\n",
    "    2. If yes, use the retrieval tool\n",
    "    3. Check the relevance of retrieved documents\n",
    "    4. Either rewrite the query or generate an answer\n",
    "    5. Always cite your sources from the context\n",
    "\n",
    "    Be explicit about each step you're taking.\"\"\",\n",
    "    tools=[retrieve_documents, search_context, check_relevance, rewrite_query],\n",
    "    llm=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "def run_rag_agent(agent, messages):\n",
    "    num_init_messages = len(messages)\n",
    "    messages = messages.copy()\n",
    "\n",
    "    while True:\n",
    "        # Get tool schemas and tool map\n",
    "        tool_schemas = [function_to_schema(tool) for tool in agent.tools]\n",
    "        tools = {tool.__name__: tool for tool in agent.tools}\n",
    "\n",
    "        # Make API call with system message and history\n",
    "        response = client.chat.completions.create(\n",
    "            model=agent.llm,\n",
    "            messages=[{\"role\": \"system\", \"content\": agent.system_message}] + messages,\n",
    "            tools=tool_schemas,\n",
    "        )\n",
    "\n",
    "        # Get and append the assistant's message\n",
    "        message = response.choices[0].message\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": message.content,\n",
    "            \"tool_calls\": message.tool_calls if hasattr(message, \"tool_calls\") else None\n",
    "        })\n",
    "\n",
    "        if message.content:\n",
    "            print(f\"{agent.name}:\", message.content)\n",
    "\n",
    "        if not hasattr(message, \"tool_calls\") or not message.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Handle tool calls\n",
    "        for tool_call in message.tool_calls:\n",
    "            result = execute_tool_call(tool_call, tools, agent.name)\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": str(result),\n",
    "            })\n",
    "\n",
    "    return messages[num_init_messages:]\n",
    "\n",
    "# Main execution\n",
    "messages = []\n",
    "\n",
    "# First, index a document\n",
    "url = \"https://en.wikipedia.org/wiki/Alan_Turing\"\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Please retrieve and index this document: {url}\"\n",
    "})\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = run_rag_agent(rag_agent, messages)\n",
    "        messages.extend(response)\n",
    "\n",
    "        user_input = input(\"\\nUser (type 'quit' to exit): \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Supervisor + Workers\n",
    "\n",
    "![Image of super special artist](https://goyalpramod.github.io/assets/blog_assets/ai_agents/2.webp)\n",
    "\n",
    "> Image taken from langgraph [docs](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/)\n",
    "\n",
    "```\n",
    "# Math Genius Tools\n",
    "def solve_equation(equation: str) -> str:\n",
    "    \"\"\"\n",
    "    Solves mathematical equations.\n",
    "    Args:\n",
    "        equation: str - A mathematical equation to solve\n",
    "            e.g. \"2x + 5 = 13\" or \"integrate(x^2)\"\n",
    "    Returns:\n",
    "        str: The solution to the equation\n",
    "    \"\"\"\n",
    "    # In a real implementation, this would use a math solving library\n",
    "    return f\"Solution for equation: {equation}\"\n",
    "\n",
    "def perform_statistics(data: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs statistical analysis on given data.\n",
    "    Args:\n",
    "        data: str - Data in format \"[1,2,3,4]\" or \"1,2,3,4\"\n",
    "    Returns:\n",
    "        str: Statistical analysis including mean, median, mode, std dev\n",
    "    \"\"\"\n",
    "    # In a real implementation, this would use numpy/pandas\n",
    "    return f\"Statistical analysis of: {data}\"\n",
    "\n",
    "# Code Writer Tools\n",
    "def generate_code(specification: str, language: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates code based on specifications.\n",
    "    Args:\n",
    "        specification: str - Description of what the code should do\n",
    "        language: str - Programming language to use\n",
    "    Returns:\n",
    "        str: Generated code\n",
    "    \"\"\"\n",
    "    return f\"Generated {language} code for: {specification}\"\n",
    "\n",
    "def review_code(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Reviews code for best practices and potential issues.\n",
    "    Args:\n",
    "        code: str - Code to review\n",
    "    Returns:\n",
    "        str: Code review comments\n",
    "    \"\"\"\n",
    "    return f\"Code review for: {code}\"\n",
    "\n",
    "def transfer_to_math_genius():\n",
    "    \"\"\"Transfer to math genius agent for complex calculations.\"\"\"\n",
    "    return math_genius_agent\n",
    "\n",
    "def transfer_to_code_writer():\n",
    "    \"\"\"Transfer to code writer agent for programming tasks.\"\"\"\n",
    "    return code_writer_agent\n",
    "\n",
    "def transfer_to_supervisor():\n",
    "    \"\"\"Transfer back to supervisor agent.\"\"\"\n",
    "    return supervisor_agent\n",
    "\n",
    "# Agent Definitions\n",
    "math_genius_agent = Agent(\n",
    "    name=\"Math Genius\",\n",
    "    llm=\"gpt-4o\",\n",
    "    system_message=(\n",
    "        \"You are a mathematical genius capable of solving complex equations \"\n",
    "        \"and performing advanced statistical analysis. Always show your work \"\n",
    "        \"step by step. If a task is outside your mathematical expertise, \"\n",
    "        \"transfer back to the supervisor.\"\n",
    "    ),\n",
    "    tools=[solve_equation, perform_statistics, transfer_to_supervisor]\n",
    ")\n",
    "\n",
    "code_writer_agent = Agent(\n",
    "    name=\"Code Writer\",\n",
    "    llm=\"gpt-4o\",\n",
    "    system_message=(\n",
    "        \"You are an expert programmer capable of writing efficient, clean code \"\n",
    "        \"and providing detailed code reviews. Always explain your code and \"\n",
    "        \"include comments. If a task is outside your programming expertise, \"\n",
    "        \"transfer back to the supervisor.\"\n",
    "    ),\n",
    "    tools=[generate_code, review_code, transfer_to_supervisor]\n",
    ")\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor\",\n",
    "    llm=\"gpt-4o\",\n",
    "    system_message=(\n",
    "        \"You are a supervisor agent responsible for delegating tasks to specialized agents. \"\n",
    "        \"For mathematical problems, delegate to the Math Genius. \"\n",
    "        \"For programming tasks, delegate to the Code Writer. \"\n",
    "        \"Ensure all responses are complete and coordinate between agents when needed.\"\n",
    "    ),\n",
    "    tools=[transfer_to_math_genius, transfer_to_code_writer]\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "def process_request(user_input: str):\n",
    "    \"\"\"\n",
    "    Process user request through the multi-agent system.\n",
    "    Args:\n",
    "        user_input: str - User's question or request\n",
    "    Returns:\n",
    "        List of messages containing the conversation\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    response = run_full_turn(supervisor_agent, messages)\n",
    "    return response.messages\n",
    "\n",
    "# Interactive testing loop\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the Multi-Agent System!\")\n",
    "    print(\"You can interact with a supervisor who will delegate to either:\")\n",
    "    print(\"1. Math Genius - for mathematical problems\")\n",
    "    print(\"2. Code Writer - for programming tasks\")\n",
    "    print(\"Type 'exit', 'quit', or 'bye' to end the conversation\\n\")\n",
    "\n",
    "    agent = supervisor_agent\n",
    "    messages = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"User: \")\n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "\n",
    "            messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "            response = run_full_turn(agent, messages)\n",
    "            agent = response.agent\n",
    "            messages.extend(response.messages)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            print(\"Resetting conversation...\")\n",
    "            messages = []\n",
    "            agent = supervisor_agent\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "Now you have all the knowledge required to build complex agentic workflows using pure python.\n",
    "\n",
    "Mind you, frameworks like langchain are still extremely useful (I use them in my day job daily) but you still need to understand the underlying idea and technology behind what you are doing.\n",
    "\n",
    "There are many things that we have not discussed like token usage optimization, error recovery strategies, Security best practices etc. I hope the reader can explore about them using the links provided in references.\n",
    "\n",
    "Hope you had fun reading this. Thank you!!\n",
    "\n",
    "Cheers :)\n",
    "\n",
    "Ways to help out\n",
    "----------------\n",
    "\n",
    "*   **Share**: Consider clicking on any of the links below to share this blog, It reaches more people and they get to learn something new. Which make’s me happy :), also. Consider tagging me from my socials.\n",
    "*   **Translate**: This blog is written in english, but there are a lot of people who do not speak english. I will be really glad if you are willing to translate this work in the language you prefer. (Consider raising a PR and I will attach your translation at the top)\n",
    "*   **Corrections & Feedback**: I am human, and I too can err. If you find I have made any mistakes, again feel free to raise a PR. Or contact me through my socials.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "*   [OpenAI blog](https://cookbook.openai.com/examples/orchestrating_agents)\n",
    "*   [Lil’log’s blog on prompt engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n",
    "*   [Anthropic’s blog on building effective agents](https://www.anthropic.com/research/building-effective-agents)\n",
    "*   [Chip’s Blog on Agents](https://huyenchip.com/2025/01/07/agents.html)\n",
    "*   [Core code for Swarm](https://github.com/openai/swarm/blob/main/swarm/core.py)\n",
    "*   [HF docs on building agents](https://huggingface.co/docs/smolagents/tutorials/building_good_agents)\n",
    "*   [HF blog on smolagents](https://huggingface.co/blog/smolagents)\n",
    "*   Meme at top taken from [dilbert](https://www.reddit.com/r/ProgrammerHumor/comments/1dckq74/soundsfamiliar/)\n",
    "    return bool(re.match(pattern, email))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**Prompt Bloat**\n",
    "\n",
    "Bad:\n",
    "\n",
    "urltomarkdowncodeblockplaceholder120.6520599915030787\n",
    "\n",
    "Good:\n",
    "\n",
    "urltomarkdowncodeblockplaceholder130.3427486620101272\n",
    "\n",
    "**Tool Proliferation**\n",
    "\n",
    "Instead of:\n",
    "\n",
    "urltomarkdowncodeblockplaceholder140.6118798767990326\n",
    "\n",
    "Do this:\n",
    "\n",
    "urltomarkdowncodeblockplaceholder150.8530511937492307\n",
    "\n",
    "**Testing Framework**\n",
    "\n",
    "urltomarkdowncodeblockplaceholder160.03431562243613362\n",
    "\n",
    "Anthropic has a nice [cookbook](https://github.com/anthropics/anthropic-cookbook/blob/main/patterns/agents/evaluator_optimizer.ipynb) on creating a testing framework.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "*   **Minimize LLM Usage**: Each call costs time and money\n",
    "*   **Start Simple**: Add complexity only when needed\n",
    "*   **Measure Everything**: You can’t improve what you don’t measure\n",
    "*   **Test Thoroughly**: Include edge cases and error scenarios\n",
    "*   **Document Decisions**: Keep track of why each feature exists\n",
    "\n",
    "Building an Agent\n",
    "-----------------\n",
    "\n",
    "![Image of super special artist](https://goyalpramod.github.io/assets/blog_assets/ai_agents/6.webp)\n",
    "\n",
    "We will start simple from setting up a simple LLM call that obeys a system prompt to a full blown multi-agent setup.\n",
    "\n",
    "The code here will be for educational purpose only, to see the whole code, visit this [repo](https://github.com/goyalpramod/paper_implementations/blob/main/AI_agents_from_first_principles.ipynb).\n",
    "\n",
    "The code in this section of the blog was heavily inspired from [OpenAI Cookbook](https://cookbook.openai.com/examples/orchestrating_agents)\n",
    "\n",
    "### LLM call\n",
    "\n",
    "urltomarkdowncodeblockplaceholder170.7484660657108377\n",
    "\n",
    "I chose OpenAI as I had some credits lying around, you can do similar thing with any other api provider.\n",
    "\n",
    "Here there are 4 arguments\n",
    "\n",
    "> content -> The user query/input.  \n",
    "> system\\_message -> What do you want the LLM to do.  \n",
    "> messages -> Past messages/conversation.  \n",
    "> tool\\_schemas -> The structure of the tools\n",
    "\n",
    "Let’s give it the same prompt that we made earlier and see how it works.\n",
    "\n",
    "urltomarkdowncodeblockplaceholder180.8314332233476662\n",
    "\n",
    "This works as expected, Let’s build on top of this by giving our LLM the ability to calculate sums of numbers. (You can define the function anyhow you would like)\n",
    "\n",
    "### LLM call + Tools\n",
    "\n",
    "I mentioned earlier that Tools are nothing but functions, and these functions are sent as schema to a model. And then these models extract out the inputs to these functions from the user input and provide the required output.\n",
    "\n",
    "Let’s create a utility function that takes another function and creates it’s schema\n",
    "\n",
    "urltomarkdowncodeblockplaceholder190.41396832593153143\n",
    "\n",
    "\\_\\_name\\_\\_ is a dunder/magic function, if you have never heard of them, read more about them [here](https://realpython.com/python-magic-methods/).\n",
    "\n",
    "A function that takes another function can be simplified using a decorator. That is what one usually sees in most libraries/framework “@tool”. Which is what we talked about in the beginning.\n",
    "\n",
    "Now let’s create a sum tool.\n",
    "\n",
    "urltomarkdowncodeblockplaceholder200.5438651098447318\n",
    "\n",
    "We can make the above function a bit more dummy proof for dumber models by modifying it as such:\n",
    "\n",
    "urltomarkdowncodeblockplaceholder210.020266283280199637\n",
    "\n",
    "While we are at it, let’s create an additional multiply\\_numbers tool too.\n",
    "\n",
    "urltomarkdowncodeblockplaceholder220.99659662747972\n",
    "\n",
    "Time to use this tool with our LLM to see how well it works.\n",
    "\n",
    "urltomarkdowncodeblockplaceholder230.6733501359996032\n",
    "\n",
    "Now we have a single function that can take in instructions as well as the tools that we want, we can increase the complexity of the tools and prompts without worrying about creating an increasingly complex pipeline.\n",
    "\n",
    "Now that we can get the tool names and arguments, its time to create another utility function that can take these info and actually execute them.\n",
    "\n",
    "urltomarkdowncodeblockplaceholder240.689010487466561\n",
    "\n",
    "Now we would like our llms to take this response and send an output to the user. Let’s do that.\n",
    "\n",
    "urltomarkdowncodeblockplaceholder250.7944538404172825\n",
    "\n",
    "### Agent(LLM call + Tools + Pydantic Model)\n",
    "\n",
    "Let’s first build a model (This is the pydantic model, I will be referring to these as models. And Large Language Models as LLMs)\n",
    "\n",
    "urltomarkdowncodeblockplaceholder260.6687516943509058\n",
    "\n",
    "Now we can modify the code we wrote earlier to use this model\n",
    "\n",
    "urltomarkdowncodeblockplaceholder270.4769224050791807\n",
    "\n",
    "And just as easily we can run multiple agents\n",
    "\n",
    "urltomarkdowncodeblockplaceholder280.09963819787175887\n",
    "\n",
    "When building multi-agent systems, we want seamless transitions between agents based on the task requirements. The naive approach would be manual handoffs, but we can make this smarter.\n",
    "\n",
    "Instead of manually switching agents, we can create a handoff mechanism where agents decide themselves when to transfer control. Let’s build this:\n",
    "\n",
    "urltomarkdowncodeblockplaceholder290.15217653167802947\n",
    "\n",
    "urltomarkdowncodeblockplaceholder300.9966488780057088\n",
    "\n",
    "Popular Agentic systems\n",
    "-----------------------\n",
    "\n",
    "The above is all the knowledge you require to build more complex agentic systems, let’s explore a few of them. Solely using what we have learned so far.\n",
    "\n",
    "Langgraph has a nice list of agentic systems in my opinion, you can check them out [here](https://langchain-ai.github.io/langgraph/tutorials/#multi-agent-systems).\n",
    "\n",
    "### ReAct\n",
    "\n",
    "![Image of super special artist](https://goyalpramod.github.io/assets/blog_assets/ai_agents/4.webp)\n",
    "\n",
    "> Image taken from the [ReAct paper](https://arxiv.org/abs/2210.03629)\n",
    "\n",
    "urltomarkdowncodeblockplaceholder310.4363120451510176\n",
    "\n",
    "urltomarkdowncodeblockplaceholder320.7701884907385055\n",
    "\n",
    "urltomarkdowncodeblockplaceholder330.7773334292691088\n",
    "\n",
    "### Agentic RAG\n",
    "\n",
    "![Image of super special artist](https://goyalpramod.github.io/assets/blog_assets/ai_agents/3.webp)\n",
    "\n",
    "> Image taken from langgraph [docs](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/)\n",
    "\n",
    "urltomarkdowncodeblockplaceholder340.7728528706610271\n",
    "\n",
    "### Supervisor + Workers\n",
    "\n",
    "![Image of super special artist](https://goyalpramod.github.io/assets/blog_assets/ai_agents/2.webp)\n",
    "\n",
    "> Image taken from langgraph [docs](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/)\n",
    "\n",
    "urltomarkdowncodeblockplaceholder350.46926373011835665\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "Now you have all the knowledge required to build complex agentic workflows using pure python.\n",
    "\n",
    "Mind you, frameworks like langchain are still extremely useful (I use them in my day job daily) but you still need to understand the underlying idea and technology behind what you are doing.\n",
    "\n",
    "There are many things that we have not discussed like token usage optimization, error recovery strategies, Security best practices etc. I hope the reader can explore about them using the links provided in references.\n",
    "\n",
    "Hope you had fun reading this. Thank you!!\n",
    "\n",
    "Cheers :)\n",
    "\n",
    "Ways to help out\n",
    "----------------\n",
    "\n",
    "*   **Share**: Consider clicking on any of the links below to share this blog, It reaches more people and they get to learn something new. Which make’s me happy :), also. Consider tagging me from my socials.\n",
    "*   **Translate**: This blog is written in english, but there are a lot of people who do not speak english. I will be really glad if you are willing to translate this work in the language you prefer. (Consider raising a PR and I will attach your translation at the top)\n",
    "*   **Corrections & Feedback**: I am human, and I too can err. If you find I have made any mistakes, again feel free to raise a PR. Or contact me through my socials.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "*   [OpenAI blog](https://cookbook.openai.com/examples/orchestrating_agents)\n",
    "*   [Lil’log’s blog on prompt engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n",
    "*   [Anthropic’s blog on building effective agents](https://www.anthropic.com/research/building-effective-agents)\n",
    "*   [Chip’s Blog on Agents](https://huyenchip.com/2025/01/07/agents.html)\n",
    "*   [Core code for Swarm](https://github.com/openai/swarm/blob/main/swarm/core.py)\n",
    "*   [HF docs on building agents](https://huggingface.co/docs/smolagents/tutorials/building_good_agents)\n",
    "*   [HF blog on smolagents](https://huggingface.co/blog/smolagents)\n",
    "*   Meme at top taken from [dilbert](https://www.reddit.com/r/ProgrammerHumor/comments/1dckq74/soundsfamiliar/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
